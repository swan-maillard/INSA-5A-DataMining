{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-03T16:55:03.498148Z",
     "iopub.status.busy": "2025-01-03T16:55:03.497721Z",
     "iopub.status.idle": "2025-01-03T16:55:03.502661Z",
     "shell.execute_reply": "2025-01-03T16:55:03.501571Z",
     "shell.execute_reply.started": "2025-01-03T16:55:03.498078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional, List, Callable, Any, Union, Dict\n",
    "from itertools import product\n",
    "from statistics import mean\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read datasets\n",
    "Use the gzip function is files ar gzipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T16:55:03.504167Z",
     "iopub.status.busy": "2025-01-03T16:55:03.503845Z",
     "iopub.status.idle": "2025-01-03T16:55:03.523023Z",
     "shell.execute_reply": "2025-01-03T16:55:03.521924Z",
     "shell.execute_reply.started": "2025-01-03T16:55:03.504128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_ds_gzip(path: Optional[Path]=None, ds: str = \"TRAIN\") -> pd.DataFrame:\n",
    "    \"\"\"Args:\n",
    "        path (Optional[Path], optional): the path to read the dataset file. Defaults to /kaggle/input/the-insa-starcraft-2-player-prediction-challenge/{ds}.CSV.gz.\n",
    "        ds (str, optional): the part to read (TRAIN or TEST), to use when path is None. Defaults to \"TRAIN\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "    \"\"\"\n",
    "    with gzip.open(f'/kaggle/input/the-insa-starcraft-2-player-prediction-challenge/{ds}.CSV.gz' if path is None else path) as f:\n",
    "        max_actions = max(( len( str(c).split(\",\")) for c in f.readlines() ))\n",
    "        f.seek(0)\n",
    "        _names = [\"battleneturl\", \"played_race\"] if \"TRAIN\" in ds else [\"played_race\"]\n",
    "        _names.extend(range(max_actions - len(_names)))\n",
    "        return pd.read_csv(f, names=_names, dtype= str)\n",
    "\n",
    "def read_ds(path: Optional[Path]=None, ds: str = \"TRAIN\"):\n",
    "    \"\"\"Args:\n",
    "        path (Optional[Path], optional): the path to read the dataset file. Defaults to /kaggle/input/the-insa-starcraft-2-player-prediction-challenge/{ds}.CSV.gz.\n",
    "        ds (str, optional): the part to read (TRAIN or TEST), to use when path is None. Defaults to \"TRAIN\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame:\n",
    "    \"\"\"\n",
    "    with open(f'/kaggle/input/train-sc2-keystrokes/{ds}.CSV' if path is None else path) as f:\n",
    "        max_actions = max(( len( str(c).split(\",\")) for c in f.readlines() ))\n",
    "        f.seek(0)\n",
    "        _names = [\"battleneturl\", \"played_race\"] if \"TRAIN\" in ds else [\"played_race\"]\n",
    "        _names.extend(range(max_actions - len(_names)))\n",
    "        return pd.read_csv(f, names=_names, dtype= str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T16:55:03.525005Z",
     "iopub.status.busy": "2025-01-03T16:55:03.524636Z",
     "iopub.status.idle": "2025-01-03T16:55:12.650412Z",
     "shell.execute_reply": "2025-01-03T16:55:12.649188Z",
     "shell.execute_reply.started": "2025-01-03T16:55:03.524971Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "features_train = read_ds_gzip(Path(\"data/TRAIN.CSV.GZ\")) # Replace with correct path \n",
    "\n",
    "# features_test = read_ds(\"TEST\")\n",
    "features_train.shape #, features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependent Variable\n",
    "Our dependent variable is a categorical string; we can convert it to categories codes (number) with pd.Categorical\n",
    "\n",
    "pd.Categorical doesn't directly modify the battleneturl to a number, instead it adds a cat.codes attribute to it. We can create a little function to convert the dependent variable from string to its category ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T16:55:12.652394Z",
     "iopub.status.busy": "2025-01-03T16:55:12.652048Z",
     "iopub.status.idle": "2025-01-03T16:55:12.657336Z",
     "shell.execute_reply": "2025-01-03T16:55:12.655950Z",
     "shell.execute_reply.started": "2025-01-03T16:55:12.652364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def to_categories(df: pd.DataFrame, col: str=\"battleneturl\") -> None:\n",
    "    \"\"\"Convert col of df to a categorical column\"\"\"\n",
    "    df[\"battleneturl\"] = pd.Categorical(df[\"battleneturl\"])\n",
    "    df[[col]] = df[[col]].apply(lambda x: x.cat.codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing outliers\n",
    "YOUR IDEAS / APPROACHES HERE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T16:55:12.658769Z",
     "iopub.status.busy": "2025-01-03T16:55:12.658394Z",
     "iopub.status.idle": "2025-01-03T16:55:13.712063Z",
     "shell.execute_reply": "2025-01-03T16:55:13.710917Z",
     "shell.execute_reply.started": "2025-01-03T16:55:12.658728Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "to_categories(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nan_counts = features_train.isna().sum()\n",
    "nan_counts.plot()\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Number of NaN values')\n",
    "plt.title('Number of NaN values per column')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train['played_race'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Played Race')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Played Race')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "durations = []\n",
    "\n",
    "# Iterate over each row in the dataframe\n",
    "for _, row in features_train.iterrows():\n",
    "    # Convert the row to a list (strings, possibly some NaN if missing)\n",
    "    row_list = row.tolist()\n",
    "\n",
    "    # Filter columns that start with \"t\" (e.g., \"t10\", \"t120\", etc.)\n",
    "    time_tokens = [\n",
    "        col for col in row_list if isinstance(col, str) and col.startswith(\"t\")\n",
    "    ]\n",
    "\n",
    "    if time_tokens:\n",
    "        # Get the last time token in the row\n",
    "        last_time_token = time_tokens[-1]\n",
    "        # Convert from e.g. \"t135\" -> integer 135\n",
    "        duration = int(last_time_token[1:])\n",
    "        durations.append(duration)\n",
    "    else:\n",
    "        # If no time tokens, we can append None or skip.\n",
    "        durations.append(None)\n",
    "\n",
    "# Store the durations in the dataframe as a new column for convenience\n",
    "features_train[\"duration\"] = durations\n",
    "\n",
    "# Drop rows without a valid duration\n",
    "features_train.dropna(subset=[\"duration\"], inplace=True)\n",
    "\n",
    "# Plot the distribution as a histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "features_train[\"duration\"].plot(kind=\"hist\", bins=20, edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Game Durations\")\n",
    "plt.xlabel(\"Game Duration (seconds)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the central 96% (remove bottom and top 2%)\n",
    "q02 = features_train['duration'].quantile(0.02)\n",
    "q98 = features_train['duration'].quantile(0.98)\n",
    "\n",
    "features_train_filtered = features_train[(features_train['duration'] >= q02) & (features_train['duration'] <= q98)]\n",
    "    \n",
    "\n",
    "# Plot the distribution as a histogram\n",
    "plt.figure(figsize=(8, 6))\n",
    "features_train_filtered['duration'].plot(kind='hist', bins=20, edgecolor='black')\n",
    "plt.title('Distribution of Game Durations')\n",
    "plt.xlabel('Game Duration (seconds)')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display global statistics about the dataset\n",
    "print(\"Total number of rows (games):\", len(features_train_filtered))\n",
    "\n",
    "# If you have a unique player identifier (e.g. 'battleneturl')\n",
    "if \"battleneturl\" in features_train_filtered.columns:\n",
    "    print(\"Number of unique players:\", features_train_filtered[\"battleneturl\"].nunique())\n",
    "\n",
    "# If you have a race column (e.g. 'played_race')\n",
    "if \"played_race\" in features_train_filtered.columns:\n",
    "    print(\"Number of unique races:\", features_train_filtered[\"played_race\"].nunique(), \"\\n\")\n",
    "    print(\"=== Race Distribution ===\\n\", features_train_filtered[\"played_race\"].value_counts(), \"\\n\")\n",
    "\n",
    "# Basic statistics for durations\n",
    "if \"duration\" in features_train_filtered.columns:\n",
    "    print(\"=== Duration Summary ===\")\n",
    "    print(features_train_filtered[\"duration\"].agg([\"min\", \"max\", \"mean\", \"std\"]))\n",
    "\n",
    "    # Example: Group by race to see duration stats per race\n",
    "    if \"played_race\" in features_train_filtered.columns:\n",
    "        print(\"\\n=== Duration by Race ===\")\n",
    "        print(features_train_filtered.groupby(\"played_race\")[\"duration\"].agg([\"min\", \"max\", \"mean\", \"std\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duration column\n",
    "features_train_filtered.drop(columns=['duration'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a deep copy to avoid the SettingWithCopyWarning\n",
    "features_train_filtered = features_train_filtered.copy()\n",
    "\n",
    "# Now safely do your assignments\n",
    "action_cols = features_train_filtered.columns[2:]\n",
    "features_train_filtered[\"actions\"] = (\n",
    "    features_train_filtered[action_cols]\n",
    "    .apply(lambda row: [x for x in row if pd.notna(x)], axis=1)\n",
    ")\n",
    "\n",
    "features_train_filtered.drop(columns=action_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify_actions(action_list):\n",
    "    \"\"\"\n",
    "    Given a list of actions (e.g., [\"Base\", \"s\", \"s\", \"t5\", \"hotkey30\", \"t10\", ...]),\n",
    "    produce a list of tuples (start_time, end_time, actions_in_chunk).\n",
    "    If we see 't5', that means the chunk boundary is 5 seconds, etc.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_time = 0\n",
    "    chunk_actions = []\n",
    "\n",
    "    def parse_time_marker(a):\n",
    "        # e.g. 't5' -> int(\"5\")\n",
    "        return int(a[1:])\n",
    "\n",
    "    for a in action_list:\n",
    "        if a.startswith(\"t\"):\n",
    "            # We hit a time boundary\n",
    "            end_time = parse_time_marker(a)\n",
    "            if chunk_actions:\n",
    "                chunks.append((current_time, end_time, chunk_actions))\n",
    "            # Move forward\n",
    "            current_time = end_time\n",
    "            chunk_actions = []\n",
    "        else:\n",
    "            # It's a normal action\n",
    "            chunk_actions.append(a)\n",
    "\n",
    "    # If leftover actions exist after the last tXX\n",
    "    # We'll treat them as from current_time to current_time (0-length),\n",
    "    # or you can define a “max game time” if known. For now, let's store them anyway.\n",
    "    if chunk_actions:\n",
    "        chunks.append((current_time, current_time, chunk_actions))\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_per_chunk(chunks):\n",
    "    \"\"\"\n",
    "    Given a list of (start_t, end_t, actions_in_chunk),\n",
    "    classify them into early/mid/late, count relevant actions,\n",
    "    compute average in each phase.\n",
    "    \"\"\"\n",
    "    # Phase boundaries\n",
    "    EARLY_MAX = 360  # 6 minutes\n",
    "    MID_MAX = 720  # 12 minutes\n",
    "\n",
    "    # We'll accumulate total counts and total durations (in seconds) for each phase\n",
    "    phases = [\"early\", \"mid\", \"late\"]\n",
    "\n",
    "    base_keys = {\n",
    "        \"hk_created\": 0,\n",
    "        \"hk_updated\": 0,\n",
    "        \"hk_used\":    0,\n",
    "        \"base\":       0,\n",
    "        \"mineral\":    0,\n",
    "        \"other\":      0\n",
    "    }\n",
    "\n",
    "    phase_counts = {\n",
    "        ph: {\n",
    "            **{f\"hk_{i}\": 0 for i in range(10)},\n",
    "            **base_keys\n",
    "        }\n",
    "        for ph in phases\n",
    "    }\n",
    "\n",
    "    phase_durations = {ph: 0 for ph in phases}\n",
    "\n",
    "    def get_phase(t):\n",
    "        if t < EARLY_MAX:\n",
    "            return \"early\"\n",
    "        elif t < MID_MAX:\n",
    "            return \"mid\"\n",
    "        else:\n",
    "            return \"late\"\n",
    "\n",
    "    for start_t, end_t, acts in chunks:\n",
    "        # Simple approach: classify chunk by its start time (some people use midpoint)\n",
    "        phase = get_phase(start_t)\n",
    "        # Avoid zero durations => set it to at least 1 second\n",
    "        duration = max(1, end_t - start_t)\n",
    "\n",
    "        # Count actions in this chunk\n",
    "        c_hk_pressed = {f\"c_hk_{i}\": 0 for i in range(10)}\n",
    "        c_hk_created = 0\n",
    "        c_hk_updated = 0\n",
    "        c_hk_used = 0\n",
    "        c_base = 0\n",
    "        c_mineral = 0\n",
    "        c_other = 0\n",
    "\n",
    "        for a in acts:\n",
    "            if a.startswith(\"hotkey\"):\n",
    "                key = int(a[-2])\n",
    "                c_hk_pressed[f\"c_hk_{key}\"] += 1\n",
    "                # check last char if it is \"0\",\"1\",\"2\"\n",
    "                if a.endswith(\"0\"):\n",
    "                    c_hk_created += 1\n",
    "                elif a.endswith(\"1\"):\n",
    "                    c_hk_updated += 1\n",
    "                elif a.endswith(\"2\"):\n",
    "                    c_hk_used += 1\n",
    "            elif a in [\"sBase\", \"Base\"]:\n",
    "                c_base += 1\n",
    "            elif a in [\"sMineral\", \"SingleMineral\"]:\n",
    "                c_mineral += 1\n",
    "            elif a == \"s\":\n",
    "                c_other += 1\n",
    "            else:\n",
    "                # some other action not relevant here\n",
    "                pass\n",
    "\n",
    "        for i in range(10):\n",
    "            phase_counts[phase][f\"hk_{i}\"] += c_hk_pressed[f\"c_hk_{i}\"]\n",
    "        phase_counts[phase][\"hk_created\"] += c_hk_created\n",
    "        phase_counts[phase][\"hk_updated\"] += c_hk_updated\n",
    "        phase_counts[phase][\"hk_used\"] += c_hk_used\n",
    "        phase_counts[phase][\"base\"] += c_base\n",
    "        phase_counts[phase][\"mineral\"] += c_mineral\n",
    "        phase_counts[phase][\"other\"] += c_other\n",
    "\n",
    "        phase_durations[phase] += duration\n",
    "\n",
    "    # Now compute the final average for each phase\n",
    "    # e.g.  (count / total_duration_in_phase)\n",
    "    feats = {}\n",
    "    for phase in [\"early\", \"mid\", \"late\"]:\n",
    "        dur = phase_durations[phase]\n",
    "        pc = phase_counts[phase]\n",
    "        if dur == 0:\n",
    "            # no data for that phase\n",
    "            for i in range(10):\n",
    "                feats[f\"{phase}_hotkey_{i}\"] = 0\n",
    "            feats[f\"{phase}_hotkey_created\"] = 0\n",
    "            feats[f\"{phase}_hotkey_updated\"] = 0\n",
    "            feats[f\"{phase}_hotkey_used\"] = 0\n",
    "            feats[f\"{phase}_base\"] = 0\n",
    "            feats[f\"{phase}_mineral\"] = 0\n",
    "            feats[f\"{phase}_other\"] = 0\n",
    "        else:\n",
    "            for i in range(10):\n",
    "                feats[f\"{phase}_hotkey_{i}\"] = (pc[f\"hk_{i}\"] / dur)\n",
    "            feats[f\"{phase}_hotkey_created\"] = (pc[\"hk_created\"] / dur)\n",
    "            feats[f\"{phase}_hotkey_updated\"] = (pc[\"hk_updated\"] / dur)\n",
    "            feats[f\"{phase}_hotkey_used\"] = (pc[\"hk_used\"] / dur)\n",
    "            feats[f\"{phase}_base\"] = (pc[\"base\"] / dur)\n",
    "            feats[f\"{phase}_mineral\"] = (pc[\"mineral\"] / dur)\n",
    "            feats[f\"{phase}_other\"] = (pc[\"other\"] / dur)\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_features(row):\n",
    "    \"\"\"\n",
    "    row is a single Pandas row with:\n",
    "       row[\"actions\"] = list of action strings\n",
    "       row[\"battleneturl\"], row[\"played_race\"], etc. if needed\n",
    "    Returns a dict of feature_name -> value\n",
    "    \"\"\"\n",
    "    chunks = chunkify_actions(row)\n",
    "    feats = compute_features_per_chunk(chunks)\n",
    "    \n",
    "    # You can also include the player's ID or race in the returned features if desired\n",
    "    return feats\n",
    "\n",
    "# Apply to entire DataFrame\n",
    "features_df = features_train_filtered[\"actions\"].apply(compute_all_features).apply(pd.Series)\n",
    "\n",
    "print(features_df.head())\n",
    "\n",
    "# Combine with the original df if you like\n",
    "result_df = pd.concat([features_train_filtered, features_df], axis=1)\n",
    "print(result_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df[\"battleneturl\"] = features_train_filtered[\"battleneturl\"]\n",
    "features_df[\"race\"] = features_train_filtered[\"played_race\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features_df.head())\n",
    "copy_df = features_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting features...\n",
    "\n",
    "Building a mini framework to read our Dataframe and convert it to features.\n",
    "\n",
    "Now we will create features out of the dataset.\n",
    "\n",
    "FeaturesGetter iterates over an ActionsDataLoader (yield every actions between two 't[xx]') and apply a set of Feature contained in a FeaturePool. At the end, it gets metrics over the values registered by each features in the feature pool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining lambdas to convert dataset to features\n",
    "We create basic features, corresponding to the mean of each action played per timestamp plus the mean of all actions together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's ready to be put into a function that'll get all the features from the initial dataframe and return a new dataframe containing only those features. FeaturesGetter gets one extra feature from that we created, which is max_time, corresponding to the \"xx\" of the last \"txx\" seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling string\n",
    "The race_played column can only take three values; instead of converting it to categorical as we did with our dependent variable, we will instead convert it to dummy variables: we one-hot encode each race. It will not add many columns to our dataframe (only three) but will allow the decision trees to split much faster on the race (on only one binary split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T16:55:13.845825Z",
     "iopub.status.busy": "2025-01-03T16:55:13.845491Z",
     "iopub.status.idle": "2025-01-03T16:55:13.874662Z",
     "shell.execute_reply": "2025-01-03T16:55:13.873395Z",
     "shell.execute_reply.started": "2025-01-03T16:55:13.845795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_dummies(df: pd.DataFrame):\n",
    "    \"\"\"Converts textual columns to one-hot encoded vectors (one column per possible value)\"\"\"\n",
    "    df = pd.get_dummies(df, columns=[\"race\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function preprocess creates a pipeline of all the function we just implemented: it create the features, converts the race to dummy variables and the dependent variable to category codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = get_dummies(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_cols = [c for c in features_df.columns if c not in ['battleneturl']]\n",
    "X = features_df[feature_cols]\n",
    "y = features_df['battleneturl'].values\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'n_estimators': [100, 200, 300, 500, 700],\n",
    "    'max_depth': [10, 15, 20, 25, None],\n",
    "    'min_samples_split': [2, 4, 8, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', 0.33, 0.5]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,            # how many random samples of params to try\n",
    "    scoring='accuracy',   # or 'f1_macro', etc., depending on your metric\n",
    "    cv=5,                 # 5-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,            # use all CPU cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Best Params:\", random_search.best_params_)\n",
    "print(\"Best CV Accuracy:\", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test Accuracy: 92.64%\n"
     ]
    }
   ],
   "source": [
    "best_rf = random_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "import joblib\n",
    "import pickle\n",
    "with open('random_forest_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_rf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Amélioré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For model building\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    RandomizedSearchCV, \n",
    "    GridSearchCV\n",
    ")\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# We'll show XGBoost; you can similarly do LightGBM\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Optional: confusion_matrix, classification_report, etc.\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [c for c in features_df.columns if c not in ['battleneturl']]\n",
    "X = features_df[feature_cols]\n",
    "y = features_df['battleneturl'].values\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter distributions for random search\n",
    "param_distributions = {\n",
    "    'n_estimators':      [100, 200, 300, 500, 700, 1000],\n",
    "    'max_depth':         [10, 15, 20, 25, None],\n",
    "    'min_samples_split': [2, 4, 8, 10],\n",
    "    'min_samples_leaf':  [1, 2, 4],\n",
    "    'max_features':      ['sqrt', 'log2', 0.33, 0.5]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,          # how many random parameter sets to try\n",
    "    scoring='accuracy', # or f1_macro, etc. based on your goal\n",
    "    cv=5,               # 5-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,          # use all CPU cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters (RandomizedSearch):\", random_search.best_params_)\n",
    "print(\"Best CV accuracy (RandomizedSearch):\", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_random = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose these were your best params\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators':      [best_params['n_estimators'] - 100, best_params['n_estimators'], best_params['n_estimators'] + 100],\n",
    "    'max_depth':         [best_params['max_depth'] - 5, best_params['max_depth'], None] if best_params['max_depth'] else [None, 20, 25],\n",
    "    'min_samples_split': [best_params['min_samples_split'], best_params['min_samples_split'] * 2],\n",
    "    'min_samples_leaf':  [best_params['min_samples_leaf'], best_params['min_samples_leaf'] + 1],\n",
    "    'max_features':      [best_params['max_features']]\n",
    "}\n",
    "\n",
    "# Make sure no negative values or nonsensical combos slip in\n",
    "cleaned_param_grid = {}\n",
    "for k, v in param_grid.items():\n",
    "    cleaned_param_grid[k] = sorted(set([x for x in v if (x is not None and x > 0) or x is None]))\n",
    "\n",
    "grid_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=grid_rf,\n",
    "    param_grid=cleaned_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters (GridSearch):\", grid_search.best_params_)\n",
    "print(\"Best CV accuracy (GridSearch):\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth':    [6, 10, 12],\n",
    "    'learning_rate':[0.01, 0.1, 0.2],\n",
    "    'subsample':    [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_search = GridSearchCV(\n",
    "    xgb,\n",
    "    param_grid=xgb_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_search.fit(X_train, y_train)\n",
    "print(\"Best XGB params:\", xgb_search.best_params_)\n",
    "print(\"Best XGB CV accuracy:\", xgb_search.best_score_)\n",
    "\n",
    "best_xgb = xgb_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Test Accuracy: {acc_rf*100:.2f}%\")\n",
    "\n",
    "# XGBoost\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Test Accuracy: {acc_xgb*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('xgb', best_xgb),\n",
    "    ],\n",
    "    voting='hard'  # or 'soft' if you want to average predicted probabilities\n",
    ")\n",
    "\n",
    "voting_ensemble.fit(X_train, y_train)\n",
    "y_pred_voting = voting_ensemble.predict(X_test)\n",
    "acc_voting = accuracy_score(y_test, y_pred_voting)\n",
    "print(f\"Voting Ensemble Test Accuracy: {acc_voting*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "stack_ensemble = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', best_rf),\n",
    "        ('xgb', best_xgb),\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    passthrough=False,  # set to True if you want to include original features in meta-level\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stack_ensemble.fit(X_train, y_train)\n",
    "\n",
    "y_pred_stack = stack_ensemble.predict(X_test)\n",
    "acc_stack = accuracy_score(y_test, y_pred_stack)\n",
    "print(f\"Stacking Ensemble Test Accuracy: {acc_stack*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Accuracies:\")\n",
    "print(f\"  Random Forest: {acc_rf*100:.2f}%\")\n",
    "print(f\"  XGBoost:       {acc_xgb*100:.2f}%\")\n",
    "print(f\"  Voting Ens:    {acc_voting*100:.2f}%\")\n",
    "print(f\"  Stacking Ens:  {acc_stack*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6399625,
     "sourceId": 10335289,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "torch_2_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
